{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbasecondafb23470c2cea4b1c9a3bf02b6bf28061",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from daisy_API import daisy_API\n",
    "import daisy_hardware.motion_library as motion_library\n",
    "\n",
    "from logger import Logger\n",
    "from low_level_traj_gen import NN_tra_generator\n",
    "import utils\n",
    "from pytorchtools.pytorchtools import EarlyStopping\n",
    "\n",
    "\n",
    "class train_NNTG():\n",
    "    def __init__(self,  \n",
    "                num_primitive, \n",
    "                z_dim,\n",
    "                policy_output_dim, \n",
    "                policy_hidden_num, \n",
    "                policy_lr, \n",
    "                batch_size,\n",
    "                mean_std,\n",
    "                device):\n",
    "            \n",
    "        self.policy = NN_tra_generator(z_dim, policy_output_dim, policy_hidden_num, device)\n",
    "        self.policy_lr = policy_lr\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(),lr=self.policy_lr, weight_decay=2e-5)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_primitive = num_primitive\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        self.learning_step = 0\n",
    "        self.mean_std = mean_std\n",
    "\n",
    "\n",
    "        # define random z_action\n",
    "        self.z_action_all = torch.tensor(np.random.normal(0,0.2,(num_primitive,z_dim)).tolist(),requires_grad=True, device = device)\n",
    "        self.z_action_optimizer = torch.optim.Adam([self.z_action_all],lr=self.policy_lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "    def sample_phase_action(self, primitive_index, size):\n",
    "        idxs = np.random.randint(0,100, size=size)\n",
    "\n",
    "        phase = (idxs + 1) / 100.0\n",
    "        expert_action = np.empty((size, 18))\n",
    "        z_vec = np.empty((size, self.z_dim))\n",
    "        for i in range(size):\n",
    "            expert_action[i] = traj[primitive_index[i]][idxs[i]]\n",
    "        z_vec = self.z_action_all[primitive_index]\n",
    "        action_vec = torch.as_tensor(utils.normalization(expert_action,self.mean_std[0], self.mean_std[1]), device= self.device).float()\n",
    "        phase_vec = torch.as_tensor(np.reshape(phase,(size,1)), device= self.device).float()\n",
    "        \n",
    "        return phase_vec, action_vec, z_vec\n",
    "\n",
    "\n",
    "\n",
    "    def update_model(self, num_iteration, save_dir, early_stopper):\n",
    "        logger = Logger(save_dir, name = 'train')\n",
    "        for i in range(num_iteration):\n",
    "            z_index = np.random.randint(0,190, size=self.batch_size)\n",
    "            state_vec, expert_action, z_vec = self.sample_phase_action(z_index, self.batch_size)\n",
    "            pred_action = self.policy.forward(z_vec, state_vec)\n",
    "\n",
    "            policy_loss = F.mse_loss(pred_action, expert_action) \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            self.z_action_optimizer.step()\n",
    "\n",
    "            self.learning_step += 1\n",
    "            logger.log('train/model_loss', policy_loss)\n",
    "            logger.dump(self.learning_step)\n",
    "\n",
    "\n",
    "\n",
    "            if (i+1)%100 == 0:\n",
    "                for _ in range(100):\n",
    "                    z_index = np.arange(190,self.num_primitive)\n",
    "                    state_vec, expert_action, z_vec = self.sample_phase_action(z_index,self.num_primitive-190)\n",
    "                    pred_action = self.policy.forward(z_vec, state_vec)\n",
    "                    policy_loss = F.mse_loss(pred_action, expert_action) \n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    self.z_action_optimizer.step()\n",
    "                logger.log('train/val_loss', policy_loss)\n",
    "                logger.dump(self.learning_step)\n",
    "                early_stopper(policy_loss)\n",
    "\n",
    "            if early_stopper.early_stop:\n",
    "                break\n",
    "\n",
    "        \n",
    "        self.save_model(save_dir)\n",
    "\n",
    "    def save_model(self, save_dir):\n",
    "        torch.save(self.policy.state_dict(),\n",
    "                   '%s/NNTG.pt' % (save_dir) )\n",
    "    \n",
    "    def load_model(self, save_dir):\n",
    "        self.policy.load_state_dict(\n",
    "            torch.load('%s/NNTG.pt' % (save_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_total = np.empty((200, 100, 18))\n",
    "traj_total = np.empty((200, 100, 18))\n",
    "\n",
    "for i in range(1):\n",
    "    traj_data = np.load('./save_data/trial_3/exp_action_' + str(i) +'.npy')\n",
    "    for j in range(100):\n",
    "        traj_total[j+100] = traj_data[j]\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    traj_data = np.load('./save_data/trial_3/exp_action_' + str(i) +'.npy')\n",
    "    for j in range(5):\n",
    "        traj_total[(i-1)*25 + j] = traj_data[j]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.save('./save_data/expert_action_total.npy', traj_total)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the nerual network\n",
    "z_dim = 2\n",
    "num_primitive = 200\n",
    "policy_output_dim = 18\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "traj = np.load('./save_data/expert_action_total.npy')\n",
    "traj_1= traj.reshape(20000,18)\n",
    "mean_std = np.array([np.mean(traj_1, axis = 0), np.std(traj_1, axis = 0)])\n",
    "np.save('./save_data/trial_'+str(z_dim) +'/LL_mean_std.npy', mean_std)\n",
    "tra_learning = train_NNTG( num_primitive = num_primitive, \n",
    "            z_dim= z_dim,\n",
    "            policy_output_dim = policy_output_dim, \n",
    "            policy_hidden_num = 512, \n",
    "            policy_lr = 1e-3, \n",
    "            batch_size = 512,\n",
    "            mean_std = mean_std,\n",
    "            device = device)\n",
    "early_stopper = EarlyStopping(patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EarlyStopping counter: 1 out of 7\nEarlyStopping counter: 2 out of 7\nEarlyStopping counter: 3 out of 7\nEarlyStopping counter: 4 out of 7\nEarlyStopping counter: 5 out of 7\nEarlyStopping counter: 6 out of 7\nEarlyStopping counter: 7 out of 7\nZ_action after optimization tensor([[-2.5729e+00,  8.8005e-01],\n        [-2.9725e+00, -3.1238e+00],\n        [-3.0785e+00, -3.5161e-01],\n        [-3.1081e+00,  3.4954e-01],\n        [-3.0323e+00, -2.5868e+00],\n        [ 2.1611e+00,  2.0021e+00],\n        [ 2.1151e+00,  2.0746e+00],\n        [ 2.1070e+00,  2.1639e+00],\n        [ 2.0082e+00,  2.1180e+00],\n        [ 2.3785e+00,  1.9430e+00],\n        [ 2.1417e+00,  2.1357e+00],\n        [ 2.2032e+00,  2.1426e+00],\n        [ 2.0487e+00,  1.8920e+00],\n        [ 2.0934e+00,  2.1806e+00],\n        [ 2.1329e+00,  2.0460e+00],\n        [ 2.1008e+00,  2.1762e+00],\n        [ 2.0338e+00,  2.0430e+00],\n        [ 2.0616e+00,  2.1236e+00],\n        [ 2.1202e+00,  2.1618e+00],\n        [ 1.9800e+00,  1.8073e+00],\n        [ 2.0344e+00,  1.8718e+00],\n        [ 1.9045e+00,  1.7178e+00],\n        [ 2.1425e+00,  2.1077e+00],\n        [ 2.1873e+00,  2.1293e+00],\n        [ 2.0155e+00,  2.0447e+00],\n        [ 1.4844e+00, -2.1733e+00],\n        [-3.1719e+00, -1.2640e+00],\n        [ 2.7540e-01, -1.5494e+00],\n        [-3.0848e+00, -1.9970e+00],\n        [-3.1314e+00, -1.7797e+00],\n        [ 1.7066e+00,  1.2201e+00],\n        [ 2.1461e+00,  2.0492e+00],\n        [ 2.0377e+00,  2.0479e+00],\n        [ 9.8239e-01,  9.3082e-01],\n        [ 2.2471e+00,  2.0829e+00],\n        [ 7.7526e-01,  1.1471e+00],\n        [ 2.1248e+00,  2.1697e+00],\n        [ 2.2012e+00,  2.0943e+00],\n        [ 2.2256e+00,  2.1241e+00],\n        [ 2.1546e+00,  2.0388e+00],\n        [ 2.0054e+00,  1.8732e+00],\n        [ 2.1355e+00,  2.2348e+00],\n        [ 1.9504e+00,  1.6236e+00],\n        [ 2.0739e+00,  2.0068e+00],\n        [ 1.8372e+00,  1.8990e+00],\n        [ 1.7143e+00,  1.5900e+00],\n        [ 2.1530e+00,  1.9575e+00],\n        [ 2.4580e+00,  1.9413e+00],\n        [ 2.0133e+00,  2.1663e+00],\n        [ 2.3092e+00,  2.0100e+00],\n        [-1.4501e+00,  2.8352e+00],\n        [-6.7797e-01,  2.0621e+00],\n        [ 1.5314e+00, -3.0795e+00],\n        [ 2.4469e+00, -2.5089e+00],\n        [-2.2338e+00,  2.6234e+00],\n        [ 2.0311e+00,  2.2009e+00],\n        [ 2.1844e+00,  2.1140e+00],\n        [ 2.1129e+00,  2.1622e+00],\n        [ 2.1604e+00,  2.1139e+00],\n        [ 2.0463e+00,  1.8420e+00],\n        [ 2.1836e+00,  2.0994e+00],\n        [ 2.1450e+00,  2.1426e+00],\n        [ 2.1177e+00,  2.0646e+00],\n        [ 2.1582e+00,  2.1465e+00],\n        [ 2.1972e+00,  2.0932e+00],\n        [ 2.1532e+00,  2.0919e+00],\n        [ 2.0917e+00,  2.1381e+00],\n        [ 2.2678e+00,  1.8909e+00],\n        [ 2.1218e+00,  2.1477e+00],\n        [ 2.0828e+00,  2.1425e+00],\n        [ 1.5079e+00,  1.4514e+00],\n        [ 2.0967e+00,  2.1367e+00],\n        [ 2.0152e+00,  2.1434e+00],\n        [ 1.1685e+00,  9.6949e-01],\n        [ 2.1138e+00,  2.2006e+00],\n        [-2.3174e+00, -2.8306e+00],\n        [-2.4205e+00,  2.0581e+00],\n        [ 6.5358e-01, -2.2795e+00],\n        [-2.6086e+00,  2.2076e+00],\n        [-2.5076e+00, -3.1326e+00],\n        [ 1.2000e+00,  6.8432e-01],\n        [ 1.9863e+00,  1.4989e+00],\n        [ 1.2437e+00,  9.2657e-01],\n        [ 2.0729e+00,  1.9443e+00],\n        [ 2.1286e+00,  2.0993e+00],\n        [ 1.8892e+00,  7.0642e-01],\n        [ 2.1452e+00,  2.1423e+00],\n        [ 2.0021e+00,  1.9423e+00],\n        [ 2.0088e+00,  1.8395e+00],\n        [ 2.1974e+00,  2.0724e+00],\n        [ 2.0375e+00,  1.8848e+00],\n        [ 2.1988e+00,  2.0395e+00],\n        [ 2.1083e+00,  2.1055e+00],\n        [ 1.8294e+00,  1.6464e+00],\n        [ 2.2335e+00,  2.1060e+00],\n        [ 2.1285e+00,  1.9440e+00],\n        [ 2.1220e+00,  2.1477e+00],\n        [ 2.0910e+00,  2.1283e+00],\n        [ 2.1101e+00,  2.0062e+00],\n        [ 2.0267e+00,  2.1659e+00],\n        [-1.2932e-01, -1.2725e+00],\n        [-1.9457e+00, -3.3908e-02],\n        [-5.7096e-01, -1.1740e+00],\n        [ 4.3905e-01, -2.3504e-01],\n        [-2.1955e+00, -2.2482e+00],\n        [-9.7906e-01, -3.1162e-01],\n        [-1.2702e+00, -2.5995e+00],\n        [ 6.1836e-01, -2.5647e-01],\n        [-1.9195e+00,  1.2980e+00],\n        [-1.9456e+00, -1.2858e+00],\n        [ 7.5040e-02, -1.6177e+00],\n        [ 4.7482e-01, -8.5244e-01],\n        [ 1.0204e+00, -4.3209e-01],\n        [ 1.6334e-02, -2.0855e+00],\n        [-5.6694e-01,  1.1034e+00],\n        [-1.5688e+00, -2.3734e+00],\n        [-1.8875e+00, -2.1329e+00],\n        [-1.5892e+00, -1.5591e+00],\n        [-1.7604e+00, -2.4796e+00],\n        [-2.2740e+00, -1.6180e+00],\n        [-1.5870e+00, -1.3653e+00],\n        [-9.4068e-01, -2.3588e+00],\n        [-1.5133e+00,  1.6656e+00],\n        [-1.8952e+00, -2.3212e+00],\n        [-1.3640e+00, -1.4547e+00],\n        [-1.7896e+00, -1.9048e+00],\n        [ 2.3747e-01, -1.2105e+00],\n        [-7.0287e-01, -2.5451e+00],\n        [-2.0253e+00, -2.2271e+00],\n        [-1.9999e-01,  1.9795e-01],\n        [-2.3199e+00, -1.9913e+00],\n        [-5.8845e-01, -1.6612e+00],\n        [ 1.6820e-02,  4.2013e-01],\n        [-2.3409e+00, -2.0181e+00],\n        [-2.3470e-01, -6.1849e-01],\n        [-2.2979e+00, -2.0111e+00],\n        [-2.2305e+00, -2.2784e+00],\n        [-1.9600e+00, -2.0883e+00],\n        [-1.4035e+00, -2.3010e+00],\n        [-1.4941e+00, -1.7915e+00],\n        [-2.3709e+00, -1.8751e+00],\n        [ 2.0834e-01,  3.2382e-01],\n        [ 1.8592e-02, -1.3363e+00],\n        [-1.9508e+00, -2.4660e+00],\n        [ 5.2249e-01, -3.6071e-01],\n        [ 6.0703e-01, -1.0359e+00],\n        [-1.9491e+00,  1.5158e+00],\n        [-1.3247e+00,  8.9493e-01],\n        [-1.0438e+00,  3.8851e-01],\n        [-2.2985e+00, -2.1870e+00],\n        [-1.6269e-01, -1.5120e+00],\n        [-1.1876e+00, -1.6210e+00],\n        [-1.2737e+00, -2.5507e+00],\n        [-2.0269e+00, -2.2792e+00],\n        [-1.8399e+00, -2.0262e+00],\n        [-8.4185e-01, -2.3535e+00],\n        [-1.9661e+00, -2.0395e+00],\n        [-1.5722e+00, -1.7640e+00],\n        [-1.1934e+00,  2.0058e-01],\n        [-9.4527e-01,  1.2486e+00],\n        [-1.0728e+00, -2.5778e+00],\n        [-2.6488e-01, -1.3066e+00],\n        [-1.5771e+00, -1.4665e+00],\n        [-1.9127e+00,  1.5007e+00],\n        [-6.7121e-01, -2.5399e+00],\n        [-1.2551e-01, -1.0994e+00],\n        [-7.7832e-01, -9.4846e-01],\n        [ 1.4039e+00, -1.0255e+00],\n        [-1.6757e+00, -4.2831e-01],\n        [-1.8672e+00, -1.4859e+00],\n        [ 3.7437e-01, -1.1283e+00],\n        [-2.7735e-01, -2.5289e+00],\n        [-2.0262e+00,  4.0576e-02],\n        [-2.1527e+00, -2.1883e+00],\n        [-7.4622e-02,  4.6505e-01],\n        [-8.3758e-01,  1.0923e+00],\n        [ 1.0852e+00, -6.0588e-01],\n        [-2.3123e+00, -1.8331e+00],\n        [-1.9982e+00, -1.0737e+00],\n        [ 7.3417e-01, -9.6175e-01],\n        [-6.6344e-03, -1.6586e+00],\n        [-7.3666e-01, -9.3460e-01],\n        [-2.3330e+00, -2.1401e+00],\n        [-1.4223e+00,  1.6850e+00],\n        [ 2.3192e-01, -1.4484e-01],\n        [-2.1608e+00, -2.2150e+00],\n        [ 4.2209e-01, -7.5893e-01],\n        [-2.0103e+00, -1.7579e+00],\n        [-4.4010e-01,  5.1075e-01],\n        [ 4.5955e-02, -2.3670e+00],\n        [-1.7197e+00,  7.4986e-01],\n        [-2.5426e-01, -2.2952e+00],\n        [-1.7393e+00, -1.1345e+00],\n        [-2.2355e-01, -1.9352e+00],\n        [ 3.0695e-04,  3.6747e-01],\n        [-1.1540e+00, -2.5380e+00],\n        [ 7.0974e-01, -1.3360e-01],\n        [-1.4408e+00, -1.7055e+00],\n        [-1.5827e+00,  1.3596e+00],\n        [-1.7971e+00, -1.5682e+00]], requires_grad=True)\n"
    }
   ],
   "source": [
    "# # # train NNTG\n",
    "# print('Z_action before optimization', tra_learning.z_action_all)\n",
    "tra_learning.update_model(num_iteration =3000, save_dir = './save_data/trial_'+str(z_dim), early_stopper=early_stopper)\n",
    "print('Z_action after optimization', tra_learning.z_action_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}