{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from daisy_API import daisy_API\n",
    "import daisy_hardware.motion_library as motion_library\n",
    "\n",
    "from logger import Logger\n",
    "from low_level_traj_gen import NN_tra_generator\n",
    "import utils\n",
    "from pytorchtools.pytorchtools import EarlyStopping\n",
    "\n",
    "\n",
    "class train_NNTG():\n",
    "    def __init__(self,  \n",
    "                num_primitive, \n",
    "                z_dim,\n",
    "                policy_output_dim, \n",
    "                policy_hidden_num, \n",
    "                policy_lr, \n",
    "                batch_size,\n",
    "                mean_std,\n",
    "                device):\n",
    "            \n",
    "        self.policy = NN_tra_generator(z_dim, policy_output_dim, policy_hidden_num, device)\n",
    "        self.policy_lr = policy_lr\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(),lr=self.policy_lr, weight_decay=2e-5)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_primitive = num_primitive\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        self.learning_step = 0\n",
    "        self.mean_std = mean_std\n",
    "\n",
    "\n",
    "        # define random z_action\n",
    "        self.z_action_all = torch.tensor(np.random.normal(0,0.2,(num_primitive,z_dim)).tolist(),requires_grad=True, device = device)\n",
    "        self.z_action_optimizer = torch.optim.Adam([self.z_action_all],lr=self.policy_lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "    def sample_phase_action(self, primitive_index, size):\n",
    "        idxs = np.random.randint(0,100, size=size)\n",
    "\n",
    "        phase = (idxs + 1) / 100.0\n",
    "        expert_action = np.empty((size, 18))\n",
    "        z_vec = np.empty((size, self.z_dim))\n",
    "        for i in range(size):\n",
    "            expert_action[i] = traj[primitive_index[i]][idxs[i]]\n",
    "        z_vec = self.z_action_all[primitive_index]\n",
    "        action_vec = torch.as_tensor(utils.normalization(expert_action,self.mean_std[0], self.mean_std[1]), device= self.device).float()\n",
    "        phase_vec = torch.as_tensor(np.reshape(phase,(size,1)), device= self.device).float()\n",
    "        \n",
    "        return phase_vec, action_vec, z_vec\n",
    "\n",
    "\n",
    "\n",
    "    def update_model(self, num_iteration, save_dir, early_stopper):\n",
    "        logger = Logger(save_dir, name = 'train')\n",
    "        for i in range(num_iteration):\n",
    "            z_index = np.random.randint(0,95, size=self.batch_size)\n",
    "            state_vec, expert_action, z_vec = self.sample_phase_action(z_index, self.batch_size)\n",
    "            pred_action = self.policy.forward(z_vec, state_vec)\n",
    "\n",
    "            policy_loss = F.mse_loss(pred_action, expert_action) \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            self.z_action_optimizer.step()\n",
    "\n",
    "            self.learning_step += 1\n",
    "            logger.log('train/model_loss', policy_loss)\n",
    "            logger.dump(self.learning_step)\n",
    "\n",
    "\n",
    "\n",
    "            if (i+1)%100 == 0:\n",
    "                for _ in range(100):\n",
    "                    z_index = np.arange(95,self.num_primitive)\n",
    "                    state_vec, expert_action, z_vec = self.sample_phase_action(z_index,self.num_primitive-95)\n",
    "                    pred_action = self.policy.forward(z_vec, state_vec)\n",
    "                    policy_loss = F.mse_loss(pred_action, expert_action) \n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    self.z_action_optimizer.step()\n",
    "                logger.log('train/val_loss', policy_loss)\n",
    "                logger.dump(self.learning_step)\n",
    "                early_stopper(policy_loss)\n",
    "\n",
    "            if early_stopper.early_stop:\n",
    "                break\n",
    "\n",
    "        \n",
    "        self.save_model(save_dir)\n",
    "\n",
    "    def save_model(self, save_dir):\n",
    "        torch.save(self.policy.state_dict(),\n",
    "                   '%s/NNTG.pt' % (save_dir) )\n",
    "    \n",
    "    def load_model(self, save_dir):\n",
    "        self.policy.load_state_dict(\n",
    "            torch.load('%s/NNTG.pt' % (save_dir)))"
   ]
  }
 ]
}